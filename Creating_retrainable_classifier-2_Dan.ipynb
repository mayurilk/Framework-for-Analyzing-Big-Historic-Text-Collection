{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Classifier\n",
    "1. Read a json and links on each page\n",
    "2. Whenever we see a null link we check the text and use it to identify our label\n",
    "3. Collect links for these pages to build the feature vector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required folder structure\n",
    "#Folder \"Data\" should contain all json files!\n",
    "#create a folder with name  - \"LABEL_MAP\"!\n",
    "#create 2 sub folders inside LABEL MAP - 1. TRAIN !\n",
    "#                                        2. TEST !\n",
    "\n",
    "# create folder with name - \"FEATURES\"!\n",
    "#create 2 sub folders inside FEATURES also - 1. TRAIN !\n",
    "#                                            2. TEST !\n",
    "\n",
    "#create \"TEST_FOLDER\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.svm import LinearSVC\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_entity_list(file):\n",
    "    entities =[]\n",
    "    all_texts =[]\n",
    "    links=[]\n",
    "    with open(file) as fp:    \n",
    "        for l in fp:\n",
    "            if l != '':\n",
    "                tb_tokens = [l1.strip() for l1 in l.split('\\t')]\n",
    "                tokens = [l1.strip() for l1 in l.split('/')]\n",
    "                link = (\"/\".join(tokens[-2:]))\n",
    "                links.append(link)\n",
    "                text = [t.strip() for t in tb_tokens[0].split()]\n",
    "                replace_dot = [t.replace('.','') for t in text[-2:]]\n",
    "                name_bigrams = tuple(replace_dot)\n",
    "                if len(name_bigrams) == 2:\n",
    "                    all_texts.append(name_bigrams) \n",
    "                    \n",
    "    links_text = [e.split('/')[1] for e in links if e != '']\n",
    "    text_to_link = [l for l in zip(links_text,all_texts)]\n",
    "    links = [e for e in set(links) if e != '']\n",
    "    entities = [(l.split('/'))[1] for l in links]\n",
    "    return entities, all_texts,text_to_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_folders(top_folder_path, entities):\n",
    "#     for e in entities:\n",
    "#         entity_path = top_folder_path +os.sep+ e\n",
    "#         if not os.path.exists(entity_path):\n",
    "#             os.makedirs(entity_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities :  41\n",
      "Total names for matching texts :  56\n",
      "Total text to link mapping :  56\n"
     ]
    }
   ],
   "source": [
    "### update this folder name!\n",
    "\n",
    "folder_name = \"Supplementary_data\"\n",
    "entities,all_texts,text_to_link = get_entity_list(folder_name + os.sep +'161111_chicago schools.txt')\n",
    "all_texts = all_texts[:-4]\n",
    "text_to_link = text_to_link[:-4]\n",
    "print (\"Total entities : \",len(entities))\n",
    "print (\"Total names for matching texts : \",len(all_texts))\n",
    "print (\"Total text to link mapping : \",len(text_to_link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in dictionary for mapping text to enitites :  56\n"
     ]
    }
   ],
   "source": [
    "dtext={}\n",
    "for t in text_to_link:\n",
    "    dtext[t[1]] = t[0]\n",
    "print (\"Items in dictionary for mapping text to enitites : \", len(dtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chicago_school_of_medicine',\n",
       " 'Chicago_Architecture_Foundation',\n",
       " 'chicago_school_of_sewing',\n",
       " 'Chicago_school_(mathematical_analysis)',\n",
       " 'chicago_school_of_law',\n",
       " 'chicago_school_of_expression',\n",
       " 'chicago_school_of_pharmacy',\n",
       " 'chicago_school_of_psychology',\n",
       " 'chicago_school_of_arms',\n",
       " 'first_Chicago_school',\n",
       " 'chicago_school_of_fiction',\n",
       " 'chicago_school_of_normal_and_applied_art',\n",
       " 'chicago_school_of_science',\n",
       " 'chicago_school_of_osteopaty',\n",
       " 'chicago_school_of_psycho-physiology',\n",
       " 'old_Chicago_school',\n",
       " 'chicago_school_of_design',\n",
       " 'chicago_school_of_hebrew',\n",
       " 'chicago_school_of_cutting',\n",
       " 'chicago_school_of_instruction',\n",
       " 'chicago_school_of_education',\n",
       " 'chicago_school_of_dentistry',\n",
       " 'chicago_school_of_anatomy',\n",
       " 'chicago_school_of_sanitary instruction',\n",
       " 'chicago_school_of_applied_and_normal_art',\n",
       " 'chicago_school_at_arms',\n",
       " 'second_Chicago_school',\n",
       " 'chicago_school_of_thought',\n",
       " 'chicago_school_of_oratory',\n",
       " 'Chicago_school_(sociology)',\n",
       " 'Chicago_school_(literary_criticism)',\n",
       " 'chicago_school_of_art',\n",
       " 'chicago_school_of_civics',\n",
       " 'chicago_school_of_lip_reading',\n",
       " 'Chicago_school_(architecture)',\n",
       " 'chicago_school_of_domestic_science',\n",
       " 'chicago_school_of_politics',\n",
       " 'new_Chicago_school',\n",
       " 'chicago_school_of_elocution',\n",
       " 'chicago_school_of_music',\n",
       " 'Chicago_Public_Schools']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chicago_Public_Schools', ('school', 'education')),\n",
       " ('Chicago_Public_Schools', ('school', 'system')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schools')),\n",
       " ('Chicago_Public_Schools', ('school', 'inspection')),\n",
       " ('Chicago_Public_Schools', ('school', 'board')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schoolroom')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schoolma')),\n",
       " ('Chicago_Public_Schools', ('deaf', 'mutes')),\n",
       " ('Chicago_Public_Schools', ('school', 'children')),\n",
       " ('Chicago_Public_Schools', ('school', 'curriculum')),\n",
       " ('Chicago_Public_Schools', ('school', 'section')),\n",
       " ('Chicago_Public_Schools', ('school', 'house')),\n",
       " ('Chicago_Public_Schools', ('school', 'extension')),\n",
       " ('Chicago_Public_Schools', ('school', 'heating')),\n",
       " ('Chicago_Public_Schools', ('school', 'ventilation')),\n",
       " ('Chicago_school_(sociology)', ('of', 'sociology')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('of', 'mathematical')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('of', 'mathematics')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('chicago', 'math')),\n",
       " ('Chicago_school_(literary_criticism)', ('of', 'criticism')),\n",
       " ('Chicago_school_(literary_criticism)', ('of', 'literary')),\n",
       " ('Chicago_school_(architecture)', ('of', 'architects')),\n",
       " ('Chicago_Architecture_Foundation', ('architecture', 'foundation')),\n",
       " ('chicago_school_of_design', ('of', 'design')),\n",
       " ('chicago_school_of_fiction', ('of', 'fiction')),\n",
       " ('chicago_school_of_applied_and_normal_art', ('normal', 'art')),\n",
       " ('chicago_school_of_lip_reading', ('lip', 'reading')),\n",
       " ('chicago_school_of_music', ('of', 'music')),\n",
       " ('chicago_school_of_osteopaty', ('of', 'osteopathy')),\n",
       " ('chicago_school_of_sewing', ('of', 'sewing')),\n",
       " ('chicago_school_of_art', ('of', 'art')),\n",
       " ('chicago_school_of_expression', ('of', 'expression')),\n",
       " ('chicago_school_of_elocution', ('of', 'elocution')),\n",
       " ('chicago_school_of_oratory', ('of', 'oratory')),\n",
       " ('chicago_school_of_civics', ('of', 'civics')),\n",
       " ('chicago_school_of_dentistry', ('of', 'dentistry')),\n",
       " ('chicago_school_of_normal_and_applied_art', ('applied', 'art')),\n",
       " ('chicago_school_of_hebrew', ('of', 'hebrew')),\n",
       " ('chicago_school_of_anatomy', ('of', 'anatomy')),\n",
       " ('chicago_school_of_arms', ('of', 'arms')),\n",
       " ('chicago_school_of_domestic_science', ('domestic', 'science')),\n",
       " ('chicago_school_of_science', ('of', 'science')),\n",
       " ('chicago_school_of_sanitary instruction', ('sanitary', 'instruction')),\n",
       " ('chicago_school_of_instruction', ('of', 'instruction')),\n",
       " ('chicago_school_of_pharmacy', ('of', 'pharmacy')),\n",
       " ('chicago_school_of_psychology', ('of', 'psychology')),\n",
       " ('chicago_school_of_psycho-physiology', ('of', 'phycho-physiology')),\n",
       " ('chicago_school_at_arms', ('and', 'boxing')),\n",
       " ('chicago_school_of_cutting', ('of', 'cutting')),\n",
       " ('chicago_school_of_thought', ('of', 'thought')),\n",
       " ('chicago_school_of_law', ('of', 'law')),\n",
       " ('chicago_school_of_politics', ('of', 'politics')),\n",
       " ('chicago_school_of_politics', ('of', 'pol')),\n",
       " ('chicago_school_of_medicine', ('of', 'medicine')),\n",
       " ('chicago_school_of_medicine', ('of', 'med')),\n",
       " ('chicago_school_of_education', ('of', 'education'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school', 'education'),\n",
       " ('school', 'system'),\n",
       " ('chicago', 'schools'),\n",
       " ('school', 'inspection'),\n",
       " ('school', 'board'),\n",
       " ('chicago', 'schoolroom'),\n",
       " ('chicago', 'schoolma'),\n",
       " ('deaf', 'mutes'),\n",
       " ('school', 'children'),\n",
       " ('school', 'curriculum'),\n",
       " ('school', 'section'),\n",
       " ('school', 'house'),\n",
       " ('school', 'extension'),\n",
       " ('school', 'heating'),\n",
       " ('school', 'ventilation'),\n",
       " ('of', 'sociology'),\n",
       " ('of', 'mathematical'),\n",
       " ('of', 'mathematics'),\n",
       " ('chicago', 'math'),\n",
       " ('of', 'criticism'),\n",
       " ('of', 'literary'),\n",
       " ('of', 'architects'),\n",
       " ('architecture', 'foundation'),\n",
       " ('of', 'design'),\n",
       " ('of', 'fiction'),\n",
       " ('normal', 'art'),\n",
       " ('lip', 'reading'),\n",
       " ('of', 'music'),\n",
       " ('of', 'osteopathy'),\n",
       " ('of', 'sewing'),\n",
       " ('of', 'art'),\n",
       " ('of', 'expression'),\n",
       " ('of', 'elocution'),\n",
       " ('of', 'oratory'),\n",
       " ('of', 'civics'),\n",
       " ('of', 'dentistry'),\n",
       " ('applied', 'art'),\n",
       " ('of', 'hebrew'),\n",
       " ('of', 'anatomy'),\n",
       " ('of', 'arms'),\n",
       " ('domestic', 'science'),\n",
       " ('of', 'science'),\n",
       " ('sanitary', 'instruction'),\n",
       " ('of', 'instruction'),\n",
       " ('of', 'pharmacy'),\n",
       " ('of', 'psychology'),\n",
       " ('of', 'phycho-physiology'),\n",
       " ('and', 'boxing'),\n",
       " ('of', 'cutting'),\n",
       " ('of', 'thought'),\n",
       " ('of', 'law'),\n",
       " ('of', 'politics'),\n",
       " ('of', 'pol'),\n",
       " ('of', 'medicine'),\n",
       " ('of', 'med'),\n",
       " ('of', 'education')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chicago_Public_Schools', ('school', 'education')),\n",
       " ('Chicago_Public_Schools', ('school', 'system')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schools')),\n",
       " ('Chicago_Public_Schools', ('school', 'inspection')),\n",
       " ('Chicago_Public_Schools', ('school', 'board')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schoolroom')),\n",
       " ('Chicago_Public_Schools', ('chicago', 'schoolma')),\n",
       " ('Chicago_Public_Schools', ('deaf', 'mutes')),\n",
       " ('Chicago_Public_Schools', ('school', 'children')),\n",
       " ('Chicago_Public_Schools', ('school', 'curriculum')),\n",
       " ('Chicago_Public_Schools', ('school', 'section')),\n",
       " ('Chicago_Public_Schools', ('school', 'house')),\n",
       " ('Chicago_Public_Schools', ('school', 'extension')),\n",
       " ('Chicago_Public_Schools', ('school', 'heating')),\n",
       " ('Chicago_Public_Schools', ('school', 'ventilation')),\n",
       " ('Chicago_school_(sociology)', ('of', 'sociology')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('of', 'mathematical')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('of', 'mathematics')),\n",
       " ('Chicago_school_(mathematical_analysis)', ('chicago', 'math')),\n",
       " ('Chicago_school_(literary_criticism)', ('of', 'criticism')),\n",
       " ('Chicago_school_(literary_criticism)', ('of', 'literary')),\n",
       " ('Chicago_school_(architecture)', ('of', 'architects')),\n",
       " ('Chicago_Architecture_Foundation', ('architecture', 'foundation')),\n",
       " ('chicago_school_of_design', ('of', 'design')),\n",
       " ('chicago_school_of_fiction', ('of', 'fiction')),\n",
       " ('chicago_school_of_applied_and_normal_art', ('normal', 'art')),\n",
       " ('chicago_school_of_lip_reading', ('lip', 'reading')),\n",
       " ('chicago_school_of_music', ('of', 'music')),\n",
       " ('chicago_school_of_osteopaty', ('of', 'osteopathy')),\n",
       " ('chicago_school_of_sewing', ('of', 'sewing')),\n",
       " ('chicago_school_of_art', ('of', 'art')),\n",
       " ('chicago_school_of_expression', ('of', 'expression')),\n",
       " ('chicago_school_of_elocution', ('of', 'elocution')),\n",
       " ('chicago_school_of_oratory', ('of', 'oratory')),\n",
       " ('chicago_school_of_civics', ('of', 'civics')),\n",
       " ('chicago_school_of_dentistry', ('of', 'dentistry')),\n",
       " ('chicago_school_of_normal_and_applied_art', ('applied', 'art')),\n",
       " ('chicago_school_of_hebrew', ('of', 'hebrew')),\n",
       " ('chicago_school_of_anatomy', ('of', 'anatomy')),\n",
       " ('chicago_school_of_arms', ('of', 'arms')),\n",
       " ('chicago_school_of_domestic_science', ('domestic', 'science')),\n",
       " ('chicago_school_of_science', ('of', 'science')),\n",
       " ('chicago_school_of_sanitary instruction', ('sanitary', 'instruction')),\n",
       " ('chicago_school_of_instruction', ('of', 'instruction')),\n",
       " ('chicago_school_of_pharmacy', ('of', 'pharmacy')),\n",
       " ('chicago_school_of_psychology', ('of', 'psychology')),\n",
       " ('chicago_school_of_psycho-physiology', ('of', 'phycho-physiology')),\n",
       " ('chicago_school_at_arms', ('and', 'boxing')),\n",
       " ('chicago_school_of_cutting', ('of', 'cutting')),\n",
       " ('chicago_school_of_thought', ('of', 'thought')),\n",
       " ('chicago_school_of_law', ('of', 'law')),\n",
       " ('chicago_school_of_politics', ('of', 'politics')),\n",
       " ('chicago_school_of_politics', ('of', 'pol')),\n",
       " ('chicago_school_of_medicine', ('of', 'medicine')),\n",
       " ('chicago_school_of_medicine', ('of', 'med')),\n",
       " ('chicago_school_of_education', ('of', 'education'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('and', 'boxing'): 'chicago_school_at_arms',\n",
       " ('applied', 'art'): 'chicago_school_of_normal_and_applied_art',\n",
       " ('architecture', 'foundation'): 'Chicago_Architecture_Foundation',\n",
       " ('chicago', 'math'): 'Chicago_school_(mathematical_analysis)',\n",
       " ('chicago', 'schoolma'): 'Chicago_Public_Schools',\n",
       " ('chicago', 'schoolroom'): 'Chicago_Public_Schools',\n",
       " ('chicago', 'schools'): 'Chicago_Public_Schools',\n",
       " ('deaf', 'mutes'): 'Chicago_Public_Schools',\n",
       " ('domestic', 'science'): 'chicago_school_of_domestic_science',\n",
       " ('lip', 'reading'): 'chicago_school_of_lip_reading',\n",
       " ('normal', 'art'): 'chicago_school_of_applied_and_normal_art',\n",
       " ('of', 'anatomy'): 'chicago_school_of_anatomy',\n",
       " ('of', 'architects'): 'Chicago_school_(architecture)',\n",
       " ('of', 'arms'): 'chicago_school_of_arms',\n",
       " ('of', 'art'): 'chicago_school_of_art',\n",
       " ('of', 'civics'): 'chicago_school_of_civics',\n",
       " ('of', 'criticism'): 'Chicago_school_(literary_criticism)',\n",
       " ('of', 'cutting'): 'chicago_school_of_cutting',\n",
       " ('of', 'dentistry'): 'chicago_school_of_dentistry',\n",
       " ('of', 'design'): 'chicago_school_of_design',\n",
       " ('of', 'education'): 'chicago_school_of_education',\n",
       " ('of', 'elocution'): 'chicago_school_of_elocution',\n",
       " ('of', 'expression'): 'chicago_school_of_expression',\n",
       " ('of', 'fiction'): 'chicago_school_of_fiction',\n",
       " ('of', 'hebrew'): 'chicago_school_of_hebrew',\n",
       " ('of', 'instruction'): 'chicago_school_of_instruction',\n",
       " ('of', 'law'): 'chicago_school_of_law',\n",
       " ('of', 'literary'): 'Chicago_school_(literary_criticism)',\n",
       " ('of', 'mathematical'): 'Chicago_school_(mathematical_analysis)',\n",
       " ('of', 'mathematics'): 'Chicago_school_(mathematical_analysis)',\n",
       " ('of', 'med'): 'chicago_school_of_medicine',\n",
       " ('of', 'medicine'): 'chicago_school_of_medicine',\n",
       " ('of', 'music'): 'chicago_school_of_music',\n",
       " ('of', 'oratory'): 'chicago_school_of_oratory',\n",
       " ('of', 'osteopathy'): 'chicago_school_of_osteopaty',\n",
       " ('of', 'pharmacy'): 'chicago_school_of_pharmacy',\n",
       " ('of', 'phycho-physiology'): 'chicago_school_of_psycho-physiology',\n",
       " ('of', 'pol'): 'chicago_school_of_politics',\n",
       " ('of', 'politics'): 'chicago_school_of_politics',\n",
       " ('of', 'psychology'): 'chicago_school_of_psychology',\n",
       " ('of', 'science'): 'chicago_school_of_science',\n",
       " ('of', 'sewing'): 'chicago_school_of_sewing',\n",
       " ('of', 'sociology'): 'Chicago_school_(sociology)',\n",
       " ('of', 'thought'): 'chicago_school_of_thought',\n",
       " ('sanitary', 'instruction'): 'chicago_school_of_sanitary instruction',\n",
       " ('school', 'board'): 'Chicago_Public_Schools',\n",
       " ('school', 'children'): 'Chicago_Public_Schools',\n",
       " ('school', 'curriculum'): 'Chicago_Public_Schools',\n",
       " ('school', 'education'): 'Chicago_Public_Schools',\n",
       " ('school', 'extension'): 'Chicago_Public_Schools',\n",
       " ('school', 'heating'): 'Chicago_Public_Schools',\n",
       " ('school', 'house'): 'Chicago_Public_Schools',\n",
       " ('school', 'inspection'): 'Chicago_Public_Schools',\n",
       " ('school', 'section'): 'Chicago_Public_Schools',\n",
       " ('school', 'system'): 'Chicago_Public_Schools',\n",
       " ('school', 'ventilation'): 'Chicago_Public_Schools'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not run next 3 cells from build classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_links(f_type,file_name,dtext,all_texts):\n",
    "    feature_vectors_for_book = OrderedDict()\n",
    "    file_page_to_link =[]\n",
    "    with open(file_name,'r') as fp:\n",
    "        data = json.load(fp)\n",
    "        all_pages = data['pages']\n",
    "        f_name = os.path.basename(file_name)\n",
    "        f_name = f_name.split('.json')[0]\n",
    "\n",
    "        for p in all_pages:\n",
    "            label = None\n",
    "            page_id = p['pid']\n",
    "            page_keys = p.keys()\n",
    "            if 'wikifier' in page_keys:\n",
    "                all_links = len(p['wikifier'])\n",
    "                links_on_page =[]\n",
    "                for l in range(all_links):\n",
    "                    if p['wikifier'][l]['link'] != None:\n",
    "                        links_on_page.append((p['wikifier'][l]['link']).split('/')[-1])\n",
    "                    elif p['wikifier'][l]['link'] == None:\n",
    "                        links_on_page.append(None)\n",
    "                        \n",
    "                if len(links_on_page) >0:\n",
    "                    if None in links_on_page:\n",
    "                        idx = links_on_page.index(None)\n",
    "                        text = p['wikifier'][idx]['text']\n",
    "                        text = [t.replace('.','').lower() for t in text.split()]\n",
    "                        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "                        common_name = set(bigrams) & set(all_texts) \n",
    "                        if len(common_name)==1:\n",
    "                            t = list(common_name)[0]\n",
    "                            if t in dtext.keys():\n",
    "                                label = dtext[t]\n",
    "                                features = [l for l in links_on_page if l != None]\n",
    "                                feature_vectors_for_book[page_id] = (label,features)\n",
    "                                with open(\"LABEL_MAP\" + os.sep + f_type+os.sep + \"file_labels\" , 'ab') as fp1:\n",
    "                                    pickle.dump((f_name + \"_\" + page_id, label),fp1)\n",
    "            elif 'wiki' in page.keys():\n",
    "                all_links = len(p['wiki'])\n",
    "                links_on_page =[]\n",
    "                for l in range(all_links):\n",
    "                    if p['wiki'][l]['link'] != None:\n",
    "                        links_on_page.append((p['wiki'][l]['link']).split('/')[-1])\n",
    "                    elif p['wiki'][l]['link'] == None:\n",
    "                        links_on_page.append(None)\n",
    "                \n",
    "                if len(links_on_page)>0:\n",
    "                    if None in links_on_page: \n",
    "                        idx = links_on_page.index(None)\n",
    "                        text = p['wiki'][idx]['text']\n",
    "                        text = [t.replace('.','').lower() for t in text.split()]\n",
    "                        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "                        common_name = set(bigrams) & set(all_texts) \n",
    "                        if len(common_name)==1:\n",
    "                            t = list(common_name)[0]\n",
    "                            if t in dtext.keys():\n",
    "                                label = dtext[t]\n",
    "                                features = [l for l in links_on_page if l != None]\n",
    "                                feature_vectors_for_book[page_id] = (label,features)\n",
    "                                with open(\"LABEL_MAP\" + os.sep + f_type + os.sep + \"file_labels\" , 'ab') as fp1:\n",
    "                                    pickle.dump((f_name + \"_\" + page_id, label),fp1)\n",
    "                        \n",
    "    with open(\"FEATURES\" + os.sep + f_type+ os.sep + \"features_\" + f_name,'wb') as fop:\n",
    "        pickle.dump(feature_vectors_for_book,fop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def collect_features_for_each_file(f_type,files,dtext,all_texts):\n",
    "    for file_name in files:\n",
    "        #comment this if don't want to print\n",
    "        print (\"Processing  : \",file_name) \n",
    "#         collect_links(f_type,file_name,dtext,all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Folder \"Data\" should contain all json files!\n",
    "# create a folder with name  - \"LABEL_MAP\"!\n",
    "#create 2 sub folders inside LABEL MAP - 1. TRAIN !\n",
    "#                                        2. TEST !\n",
    "\n",
    "# create folder with name - \"FEATURES\"!\n",
    "#create 2 sub folders inside FEATURES also - 1. TRAIN !\n",
    "#                                            2. TEST !\n",
    "\n",
    "source_folder = \"Data\"\n",
    "files = [f1 for f1 in glob.glob(source_folder +os.sep + '*.json')]\n",
    "    \n",
    "label_file = \"LABEL_MAP\" +os.sep + \"file_labels\"\n",
    "if os.path.exists(label_file):\n",
    "    os.remove(label_file)\n",
    "    \n",
    "f_train = \"TRAIN\"\n",
    "# collect_features_for_each_file(f_train, files,dtext,all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from below again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_list(folder_name):\n",
    "    featured_files = glob.glob(folder_name + os.sep + '*')\n",
    "    print(\"Total featured files : \",len(featured_files))\n",
    "    \n",
    "    features=[]\n",
    "    labels =[]\n",
    "    for f in featured_files:\n",
    "        with open(f,'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "            for k,v in data.items():\n",
    "                f = \" \".join(x for x in v[1])\n",
    "                features.append(f)\n",
    "                labels.append(v[0])\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total featured files :  9\n"
     ]
    }
   ],
   "source": [
    "# Read feature files in train folder and build feature and true label list\n",
    "feature_folder = \"FEATURES/TRAIN\"\n",
    "features,labels = build_feature_list(feature_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape :  (4, 43)\n",
      "Number of true labels :  4\n"
     ]
    }
   ],
   "source": [
    "#Build feature matrix\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(features)\n",
    "print (\"Matrix shape : \", X.shape)\n",
    "y = np.array(labels)\n",
    "print (\"Number of true labels : \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_RF_comb(X,y):\n",
    "    random_state = [35,40,42,50,60] \n",
    "    cols = ['avg_acc','docs','features','random_state']\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    combs = product(random_state)\n",
    "    max_acc =0\n",
    "    feats = X.shape[0]\n",
    "    docs = X.shape[1]\n",
    "    for i, c in enumerate(random_state):\n",
    "        print (\"Combination  : \",i+1)\n",
    "        clf = RandomForestClassifier(random_state = c, class_weight = 'balanced')\n",
    "        clf.fit(X, labels)\n",
    "        acc = cross_val_score(clf,X,y,cv =5)\n",
    "        avg_acc = np.mean(acc)\n",
    "        print (\"Accuracies :\",acc)\n",
    "        print (\"Average accuracy : \",avg_acc)\n",
    "        val = [avg_acc,docs,feats,c]\n",
    "        df.loc[i] = val\n",
    "        if avg_acc > max_acc:\n",
    "            max_acc = avg_acc\n",
    "            combi = c\n",
    "    print('\\nMax accuracy : ', max_acc)\n",
    "    print('Best Combination : ',combi)\n",
    "    df = df.sort_values(by='avg_acc', ascending=False)\n",
    "\n",
    "    df.to_csv('resultsRF.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    with open('accuraciesRF.txt','a') as fp:\n",
    "        fp.write('\\nMax accuracy of RF: '+ str(max_acc))\n",
    "        fp.write('\\nCorresponding random state of RF : ' + str(combi))\n",
    "\n",
    "    return combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination  :  1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: 4.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6830eee45ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombi_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_RF_comb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e19fc788d375>\u001b[0m in \u001b[0;36mfind_RF_comb\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracies :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n\u001b[0;32m--> 140\u001b[0;31m                       for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchedCalls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterator_slice)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[1;32m    136\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[0;32m--> 137\u001b[0;31m     scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,\n\u001b[0m\u001b[1;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                                               fit_params)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 (\"Cannot have number of splits n_splits={0} greater\"\n\u001b[1;32m    318\u001b[0m                  \u001b[0;34m\" than the number of samples: {1}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                                                              n_samples))\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: 4."
     ]
    }
   ],
   "source": [
    "combi_RF = find_RF_comb(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Collecting test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_links_test_data(f_type,file_name,dtext,all_texts):\n",
    "    feature_vectors_for_book = OrderedDict()\n",
    "    file_page_to_link =[]\n",
    "    \n",
    "    with open(file_name,'r') as fp:\n",
    "        data = json.load(fp)\n",
    "        all_pages = data['pages']\n",
    "        f_name = os.path.basename(file_name)\n",
    "        f_name = f_name.split('.json')[0]\n",
    "\n",
    "        for p in all_pages:\n",
    "            label = None\n",
    "            page_id = p['pid']\n",
    "            page_keys = p.keys()\n",
    "            if 'wikifier' in page_keys:\n",
    "                all_links = len(p['wikifier'])\n",
    "                links_on_page =[]\n",
    "                for l in range(all_links):\n",
    "                    if p['wikifier'][l]['link'] != None:\n",
    "                        links_on_page.append((p['wikifier'][l]['link']).split('/')[-1])\n",
    "                    elif p['wikifier'][l]['link'] == None:\n",
    "                        links_on_page.append(None)\n",
    "                        \n",
    "                if len(links_on_page) >0:\n",
    "                    if None in links_on_page:\n",
    "                        idx = links_on_page.index(None)\n",
    "                        text = p['wikifier'][idx]['text']\n",
    "                        text = [t.replace('.','').lower() for t in text.split()]\n",
    "                        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "                        common_name = set(bigrams) & set(all_texts) \n",
    "                        if len(common_name)==0:\n",
    "                            features = [l for l in links_on_page if l != None]\n",
    "                            feature_vectors_for_book[page_id] = features\n",
    "                            with open(\"LABEL_MAP\" + os.sep + f_type+os.sep + \"file_labels\" , 'ab') as fp1:\n",
    "                                pickle.dump((f_name + \"_\" + page_id),fp1)\n",
    "            elif 'wiki' in page.keys():\n",
    "                all_links = len(p['wiki'])\n",
    "                links_on_page =[]\n",
    "                for l in range(all_links):\n",
    "                    if p['wiki'][l]['link'] != None:\n",
    "                        links_on_page.append((p['wiki'][l]['link']).split('/')[-1])\n",
    "                    elif p['wiki'][l]['link'] == None:\n",
    "                        links_on_page.append(None)\n",
    "                \n",
    "                if len(links_on_page)>0:\n",
    "                    if None in links_on_page: \n",
    "                        idx = links_on_page.index(None)\n",
    "                        text = p['wiki'][idx]['text']\n",
    "                        text = [t.replace('.','').lower() for t in text.split()]\n",
    "                        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "                        common_name = set(bigrams) & set(all_texts) \n",
    "                        if len(common_name)==0:\n",
    "                            features = [l for l in links_on_page if l != None]\n",
    "                            feature_vectors_for_book[page_id] = features\n",
    "                            with open(\"LABEL_MAP\" + os.sep + f_type+os.sep + \"file_labels\" , 'ab') as fp1:\n",
    "                                pickle.dump((f_name + \"_\" + page_id),fp1)\n",
    "                                \n",
    "    with open(\"FEATURES\" + os.sep + f_type+ os.sep + \"features_\" + f_name,'wb') as fop:\n",
    "        pickle.dump(feature_vectors_for_book,fop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_list_test_data(folder_name):\n",
    "    featured_files = glob.glob(folder_name + os.sep + '*')\n",
    "    print(\"Total featured test files : \",len(featured_files))\n",
    "    \n",
    "    test_features=[]\n",
    "    test_feature_keys=[]\n",
    "    labels =[]\n",
    "    for f in featured_files:\n",
    "        with open(f,'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "            for k,v in data.items():\n",
    "                f = \" \".join(x for x in v)\n",
    "                test_features.append(f)\n",
    "                test_feature_keys.append(f+ \"_\"+k)\n",
    "    return test_features,test_feature_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_file(file_list,source_folder,target_folder):\n",
    "    for file_name in file_list:\n",
    "        source_location = source_folder + os.sep + file_name\n",
    "        dest_location = target_folder + os.sep + file_name\n",
    "        if (os.path.exists(dest_location)) == False:\n",
    "            shutil.copy(source_location, dest_location)\n",
    "        else:\n",
    "            print (\"file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  files :  9\n",
      "Total processed files :  9\n"
     ]
    }
   ],
   "source": [
    "source_folder = \"Data\"\n",
    "all_files = [os.path.basename(f1) for f1 in glob.glob(source_folder +os.sep + '*.json')]\n",
    "print(\"Total  files : \",len(all_files))\n",
    "\n",
    "featured_folder = \"FEATURES/TRAIN\"\n",
    "processed_files = [os.path.basename(file_name) for file_name in glob.glob(featured_folder + os.sep + '*')]\n",
    "print(\"Total processed files : \",len(processed_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  files :  9\n",
      "Processing test data file :  Data/1904_chi.098235568.json\n",
      "Processing test data file :  Data/1914_chi.098235576.json\n",
      "Processing test data file :  Data/mdp.39015062797538.json\n",
      "Processing test data file :  Data/mdp.39015064582888.json\n",
      "Processing test data file :  Data/nnc1.ar53666712.json\n",
      "Processing test data file :  Data/uc1.$b715276.json\n",
      "Processing test data file :  Data/uc1.b3819355.json\n",
      "Processing test data file :  Data/uiug.30112078740336.json\n",
      "Processing test data file :  Data/uva.x001197927.json\n"
     ]
    }
   ],
   "source": [
    "# Collect features for test files \n",
    "f_test = \"TEST\"\n",
    "source_folder = \"Data\"\n",
    "# os.path.basename(f1)\n",
    "all_files = [f1 for f1 in glob.glob(source_folder +os.sep + '*.json')]\n",
    "print(\"Total files : \",len(all_files))\n",
    "# all_files = all_files[1:]\n",
    "for f in all_files:\n",
    "    print(\"Processing test data file : \", f)\n",
    "    collect_links_test_data(f_test,f,dtext,all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed test files :  9\n"
     ]
    }
   ],
   "source": [
    "test_feature_folder = \"FEATURES/TEST\"\n",
    "processed_files_test = [os.path.basename(file_name) for file_name in glob.glob(test_feature_folder + os.sep + '*')]\n",
    "print(\"Total processed test files : \",len(processed_files_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting on files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total featured test files :  9\n"
     ]
    }
   ],
   "source": [
    "# Read feature vectors from test feature files\n",
    "test_feature_folder = \"FEATURES/TEST\"\n",
    "test_features_RF,test_feature_keys_RF = build_feature_list_test_data(test_feature_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combi_RF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b69249e30860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombi_RF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRF_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_RF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRF_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combi_RF' is not defined"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = combi_RF, class_weight = 'balanced')\n",
    "clf.fit(X,y)\n",
    "RF_X_test = vec.transform(test_features_RF)\n",
    "RF_predicted = clf.predict(RF_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RF_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-67bbbbfa56be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRF_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RF_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "print (RF_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RF_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a9545e4d25ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_prediction_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feature_keys_RF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRF_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_prediction_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RF_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "file_prediction_map = [(i[0],i[1]) for i in zip(test_feature_keys_RF, RF_predicted)]\n",
    "print (file_prediction_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_prediction_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d882ae6107fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RF_prediction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prediction_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file_prediction_map' is not defined"
     ]
    }
   ],
   "source": [
    "with open('RF_prediction','w') as fp:\n",
    "    pickle.dump(file_prediction_map,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
